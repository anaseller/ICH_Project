# Географ хочет находить расстояние между двумя точками на карте.
# Напишите для него программу, которая запрашивает у пользователя
# координаты двух точек в двумерном пространстве (x1, y1) и (x2, y2),
# а затем вычисляет и выводит расстояние между этими точками по формуле:
#
# distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)
# где sqrt - функция извлечения квадратного корня. Не используйте встроенную
# математическую функцию sqrt для вычисления корня. Не забывайте, что sqrt(x)==x**0.5.
# Результат должен быть выведен с помощью команды print.
#
# Пример вывода:
# Введите координаты первой точки (x1, y1): 2, 3
# Введите координаты второй точки (x2, y2): 5, 7
# Расстояние между точками: 5.0

x1 = float(input('Введите x1: '))
y1 = float(input('Введите y1: '))
x2 = float(input('Введите x2: '))
y2 = float(input('Введите y2: '))
distance = ((x2 - x1)**2 + (y2 - y1)**2)**0.5
print('Расстояние между точками:', distance)